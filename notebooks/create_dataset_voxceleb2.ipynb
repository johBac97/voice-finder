{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import transformers\n",
    "import zarr\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import torchaudio\n",
    "import multiprocessing as mp\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of speakers:\t6114\n",
      "Number of audio clips:\t1092009\n",
      "Number of english audio clips:\t537134\n"
     ]
    }
   ],
   "source": [
    "root_dir = Path.cwd()\n",
    "\n",
    "data_folder = root_dir / \"data\" / \"voxceleb\"\n",
    "\n",
    "audio_files_path = data_folder / \"dev\" / \"aac\"\n",
    "\n",
    "clip_langauge_path = data_folder / \"audio_clips_meta_data.csv\"\n",
    "speaker_path = data_folder / \"vox2_meta.csv\"\n",
    "\n",
    "zarr_archive_path = root_dir / \"data\" / \"preprocessed\" / \"voxceleb2_v1.zarr\"\n",
    "\n",
    "zarr_archive_path.parent.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "speaker_df = pd.read_csv(speaker_path, delimiter=\"\\t\")\n",
    "language_df = pd.read_csv(clip_langauge_path, index_col=False).drop(\n",
    "    columns=[\"Unnamed: 0\"]\n",
    ")\n",
    "\n",
    "print(f\"Number of speakers:\\t{speaker_df.shape[0]}\")\n",
    "\n",
    "print(f\"Number of audio clips:\\t{language_df.shape[0]}\")\n",
    "print(\n",
    "    f\"Number of english audio clips:\\t{language_df[language_df.language == 'en'].shape[0]}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8416/3948336434.py:9: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda x: x.sample(n=n_samples_per_speaker, random_state=1337))\n"
     ]
    }
   ],
   "source": [
    "n_samples_per_speaker = 3\n",
    "\n",
    "\n",
    "language_df = (\n",
    "    language_df[language_df.language == \"en\"]\n",
    "    .groupby(\"speaker_id\")\n",
    "    .filter(lambda x: len(x) >= n_samples_per_speaker)\n",
    "    .groupby(\"speaker_id\")\n",
    "    .apply(lambda x: x.sample(n=n_samples_per_speaker, random_state=1337))\n",
    "    .reset_index(drop=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples:\t12756\n",
      "Number of unique speakers:\t4252\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>language</th>\n",
       "      <th>client_id</th>\n",
       "      <th>clip_id</th>\n",
       "      <th>audio_file</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>en</td>\n",
       "      <td>id00012</td>\n",
       "      <td>_raOc3-IRsw</td>\n",
       "      <td>00114.m4a</td>\n",
       "      <td>id00012/_raOc3-IRsw/00114.m4a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>en</td>\n",
       "      <td>id00012</td>\n",
       "      <td>Z-G8-wqpxwU</td>\n",
       "      <td>00097.m4a</td>\n",
       "      <td>id00012/Z-G8-wqpxwU/00097.m4a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>en</td>\n",
       "      <td>id00012</td>\n",
       "      <td>C_FAL9gv8bo</td>\n",
       "      <td>00021.m4a</td>\n",
       "      <td>id00012/C_FAL9gv8bo/00021.m4a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>en</td>\n",
       "      <td>id00016</td>\n",
       "      <td>mW9EXHGCHi4</td>\n",
       "      <td>00127.m4a</td>\n",
       "      <td>id00016/mW9EXHGCHi4/00127.m4a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>en</td>\n",
       "      <td>id00016</td>\n",
       "      <td>29NOrEy8ZY0</td>\n",
       "      <td>00004.m4a</td>\n",
       "      <td>id00016/29NOrEy8ZY0/00004.m4a</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  language client_id      clip_id audio_file                           path\n",
       "0       en   id00012  _raOc3-IRsw  00114.m4a  id00012/_raOc3-IRsw/00114.m4a\n",
       "1       en   id00012  Z-G8-wqpxwU  00097.m4a  id00012/Z-G8-wqpxwU/00097.m4a\n",
       "2       en   id00012  C_FAL9gv8bo  00021.m4a  id00012/C_FAL9gv8bo/00021.m4a\n",
       "3       en   id00016  mW9EXHGCHi4  00127.m4a  id00016/mW9EXHGCHi4/00127.m4a\n",
       "4       en   id00016  29NOrEy8ZY0  00004.m4a  id00016/29NOrEy8ZY0/00004.m4a"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"Number of samples:\\t{language_df.shape[0]}\")\n",
    "print(f\"Number of unique speakers:\\t{language_df.speaker_id.unique().shape[0]}\")\n",
    "language_df = language_df.rename(columns={\"speaker_id\": \"client_id\"})\n",
    "language_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_size = 80\n",
    "chunk_length = 30\n",
    "hop_length = 160\n",
    "target_sample_rate = 16000\n",
    "feature_extractor = transformers.WhisperFeatureExtractor(\n",
    "    feature_size=feature_size,\n",
    "    chunk_length=chunk_length,\n",
    "    hop_length=hop_length,\n",
    "    device=\"cuda\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/john/mp/voice-finder/.venv/lib/python3.11/site-packages/numpy/_core/fromnumeric.py:57: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n",
      "100%|██████████████████████████████████████████████████████████████████| 100/100 [05:12<00:00,  3.12s/it]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "\n",
    "zarr_root = zarr.open(zarr_archive_path, mode=\"w\")\n",
    "\n",
    "df = language_df\n",
    "\n",
    "zarr_root.create_array(\n",
    "    \"features\",\n",
    "    shape=(\n",
    "        df.shape[0],\n",
    "        feature_size,\n",
    "        int(target_sample_rate / hop_length * chunk_length) // 3,\n",
    "    ),\n",
    "    chunks=(1, feature_size, int(target_sample_rate / hop_length * chunk_length) // 3),\n",
    "    dtype=\"float32\",\n",
    ")\n",
    "\n",
    "zarr_root.create_array(\n",
    "    \"attention_mask\",\n",
    "    shape=(\n",
    "        df.shape[0],\n",
    "        int(target_sample_rate / hop_length * chunk_length) // 3,\n",
    "    ),\n",
    "    chunks=(1, int(target_sample_rate / hop_length * chunk_length) // 3),\n",
    "    dtype=\"int32\",\n",
    ")\n",
    "\n",
    "zarr_root.create_array(\n",
    "    \"client_index\",\n",
    "    shape=(df.shape[0],),\n",
    "    dtype=\"int32\",\n",
    ")\n",
    "\n",
    "zarr_root.create_array(\n",
    "    \"client_id\",\n",
    "    shape=(df.shape[0],),\n",
    "    dtype=f\"S{len(df.iloc[0])}\",\n",
    ")\n",
    "\n",
    "zarr_root.create_array(\n",
    "    \"path\",\n",
    "    shape=(df.shape[0],),\n",
    "    dtype=f\"S32\",\n",
    ")\n",
    "\n",
    "client_indices = {\n",
    "    v: k\n",
    "    for k, v in df.groupby(\"client_id\")\n",
    "    .count()\n",
    "    .reset_index()[[\"client_id\"]]\n",
    "    .to_dict()[\"client_id\"]\n",
    "    .items()\n",
    "}\n",
    "\n",
    "time_pre = []\n",
    "time_feature = []\n",
    "time_after = []\n",
    "\n",
    "\n",
    "def process_batch(batch_inputs):\n",
    "    batch_idx, batch = batch_inputs\n",
    "\n",
    "    audio_16k = []\n",
    "    row_indices = []\n",
    "    batch_client_ids = []\n",
    "    batch_age_numbers = []\n",
    "    batch_genders = []\n",
    "    client_ids = []\n",
    "    paths = []\n",
    "\n",
    "    time_pre_start = time.time()\n",
    "\n",
    "    for row_index, (index, row) in enumerate(batch.iterrows()):\n",
    "        audio_path = audio_files_path / row.client_id / row.clip_id / row.audio_file\n",
    "\n",
    "        # row_index += batch_idx * batch_size\n",
    "        row_index = index\n",
    "\n",
    "        row_indices.append(index)\n",
    "\n",
    "        if not audio_path:\n",
    "            print(f\"Missing audio file:\\t{row.path}\")\n",
    "            continue\n",
    "\n",
    "        audio, orig_sr = torchaudio.load(audio_path)\n",
    "\n",
    "        audio_16k.append(\n",
    "            torchaudio.functional.resample(\n",
    "                audio, orig_freq=orig_sr, new_freq=target_sample_rate\n",
    "            )[0, :]\n",
    "        )\n",
    "\n",
    "        batch_client_ids.append(row.client_id)\n",
    "\n",
    "        client_ids.append(row.client_id.encode(\"utf-8\"))\n",
    "        paths.append(row.path.ljust(32).encode(\"utf-8\"))\n",
    "\n",
    "    audio_max_length = max(len(w) for w in audio_16k)\n",
    "\n",
    "    audio_padded = np.array(\n",
    "        [\n",
    "            np.pad(\n",
    "                w, (0, audio_max_length - len(w)), mode=\"constant\", constant_values=0\n",
    "            )\n",
    "            for w in audio_16k\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    time_feature_start = time.time()\n",
    "    time_pre.append(time_feature_start - time_pre_start)\n",
    "\n",
    "    features = feature_extractor(\n",
    "        audio_padded,\n",
    "        return_tensors=\"np\",\n",
    "        return_attention_mask=True,\n",
    "        sampling_rate=target_sample_rate,\n",
    "        device=\"cuda\",\n",
    "    )\n",
    "\n",
    "    time_after_start = time.time()\n",
    "    time_feature.append(time_after_start - time_feature_start)\n",
    "\n",
    "    zarr_root[\"features\"][row_indices[0] : row_indices[-1] + 1, :] = features[\n",
    "        \"input_features\"\n",
    "    ][:, :, :1000]\n",
    "    zarr_root[\"attention_mask\"][row_indices[0] : row_indices[-1] + 1, :] = features[\n",
    "        \"attention_mask\"\n",
    "    ][:, :1000]\n",
    "\n",
    "    zarr_root[\"client_index\"][row_indices[0] : row_indices[-1] + 1] = np.array(\n",
    "        [client_indices[x] for x in batch_client_ids]\n",
    "    )\n",
    "\n",
    "    zarr_root[\"client_id\"][row_indices[0] : row_indices[-1] + 1] = np.array(client_ids)\n",
    "    zarr_root[\"path\"][row_indices[0] : row_indices[-1] + 1] = np.array(paths)\n",
    "\n",
    "    time_after_end = time.time()\n",
    "\n",
    "    time_after.append(time_after_end - time_after_start)\n",
    "\n",
    "\n",
    "batches = np.array_split(df.reset_index(), np.ceil(df.shape[0] / batch_size))\n",
    "\n",
    "for batch_idx, batch in tqdm(enumerate(batches), total=len(batches)):\n",
    "    process_batch((batch_idx, batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before:\t1.2836652660369874\n",
      "feature:\t1.1886273860931396\n",
      "after:\t0.6267783164978027\n"
     ]
    }
   ],
   "source": [
    "print(f\"Before:\\t{sum(time_pre) / len(time_pre)}\")\n",
    "print(f\"feature:\\t{sum(time_feature) / len(time_feature)}\")\n",
    "print(f\"after:\\t{sum(time_after) / len(time_after)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
