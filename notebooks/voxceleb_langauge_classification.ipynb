{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:speechbrain.utils.quirks:Applied quirks (see `speechbrain.utils.quirks`): [disable_jit_profiling, allow_tf32]\n",
      "INFO:speechbrain.utils.quirks:Excluded quirks specified by the `SB_DISABLE_QUIRKS` environment (comma-separated list): []\n"
     ]
    }
   ],
   "source": [
    "import torchaudio\n",
    "\n",
    "from speechbrain.inference.classifiers import EncoderClassifier\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of persons:\t6114\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>VoxCeleb2 ID</th>\n",
       "      <th>VGGFace2 ID</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Set</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Aaron_Ashmore</td>\n",
       "      <td>id00012</td>\n",
       "      <td>n000012</td>\n",
       "      <td>m</td>\n",
       "      <td>dev</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Aaron_Motsoaledi</td>\n",
       "      <td>id00015</td>\n",
       "      <td>n000015</td>\n",
       "      <td>m</td>\n",
       "      <td>dev</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Aaron_Ramsey</td>\n",
       "      <td>id00016</td>\n",
       "      <td>n000016</td>\n",
       "      <td>m</td>\n",
       "      <td>dev</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Aaron_Rodgers</td>\n",
       "      <td>id00017</td>\n",
       "      <td>n000017</td>\n",
       "      <td>m</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Aaron_Schock</td>\n",
       "      <td>id00018</td>\n",
       "      <td>n000018</td>\n",
       "      <td>m</td>\n",
       "      <td>dev</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Name  VoxCeleb2 ID  VGGFace2 ID  Gender    Set \n",
       "0     Aaron_Ashmore       id00012      n000012       m    dev \n",
       "1  Aaron_Motsoaledi       id00015      n000015       m    dev \n",
       "2      Aaron_Ramsey       id00016      n000016       m    dev \n",
       "3     Aaron_Rodgers       id00017      n000017       m   test \n",
       "4      Aaron_Schock       id00018      n000018       m    dev "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "project_root = Path.cwd().absolute()\n",
    "\n",
    "data_root = project_root / \"data\" / \"voxceleb\"\n",
    "\n",
    "audio_files = data_root / \"dev\" / \"aac\"\n",
    "\n",
    "\n",
    "meta_data_path = data_root / \"vox2_meta.csv\"\n",
    "\n",
    "meta_df = pd.read_csv(meta_data_path, delimiter=\"\\t\")\n",
    "\n",
    "print(f\"Number of persons:\\t{meta_df.shape[0]}\")\n",
    "meta_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:speechbrain.utils.fetching:Fetch hyperparams.yaml: Fetching from HuggingFace Hub 'speechbrain/lang-id-voxlingua107-ecapa' if not cached\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5715e4d318d4701a8d814d212a33d16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "hyperparams.yaml:   0%|          | 0.00/1.52k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:speechbrain.utils.fetching:Fetch custom.py: Fetching from HuggingFace Hub 'speechbrain/lang-id-voxlingua107-ecapa' if not cached\n",
      "/home/john/mp/voice-finder/.venv/lib/python3.11/site-packages/speechbrain/utils/autocast.py:68: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  wrapped_fwd = torch.cuda.amp.custom_fwd(fwd, cast_inputs=cast_inputs)\n",
      "INFO:speechbrain.utils.fetching:Fetch embedding_model.ckpt: Fetching from HuggingFace Hub 'speechbrain/lang-id-voxlingua107-ecapa' if not cached\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "441d8adef1ba4145a2fb6cffa5272816",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "embedding_model.ckpt:   0%|          | 0.00/84.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:speechbrain.utils.fetching:Fetch classifier.ckpt: Fetching from HuggingFace Hub 'speechbrain/lang-id-voxlingua107-ecapa' if not cached\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44a4f7559d8d48e7a13319d01de782a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "classifier.ckpt:   0%|          | 0.00/763k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:speechbrain.utils.fetching:Fetch label_encoder.txt: Fetching from HuggingFace Hub 'speechbrain/lang-id-voxlingua107-ecapa' if not cached\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac0d27bcfb0e475eaa2a52c4b4c8641a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "label_encoder.txt:   0%|          | 0.00/2.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:speechbrain.utils.parameter_transfer:Loading pretrained files for: embedding_model, classifier, label_encoder\n"
     ]
    }
   ],
   "source": [
    "language_id = EncoderClassifier.from_hparams(\n",
    "    source=\"speechbrain/lang-id-voxlingua107-ecapa\",\n",
    "    savedir=\"tmp\",\n",
    "    run_opts={\"device\": \"cuda\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                            | 0/5994 [00:02<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[70]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     11\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m clip_folder \u001b[38;5;129;01min\u001b[39;00m speaker_folder.iterdir():\n\u001b[32m     12\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m clip \u001b[38;5;129;01min\u001b[39;00m clip_folder.iterdir():\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m             prediction_raw = \u001b[43mlanguage_id\u001b[49m\u001b[43m.\u001b[49m\u001b[43mclassify_file\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mclip\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m             predictions.append(\n\u001b[32m     17\u001b[39m                 {\n\u001b[32m     18\u001b[39m                     \u001b[33m\"\u001b[39m\u001b[33mlanguage\u001b[39m\u001b[33m\"\u001b[39m: prediction_raw[\u001b[32m3\u001b[39m][\u001b[32m0\u001b[39m].split(\u001b[33m\"\u001b[39m\u001b[33m:\u001b[39m\u001b[33m\"\u001b[39m)[\u001b[32m0\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m     23\u001b[39m                 }\n\u001b[32m     24\u001b[39m             )\n\u001b[32m     25\u001b[39m predictions_df = pd.DataFrame(predictions)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/mp/voice-finder/.venv/lib/python3.11/site-packages/speechbrain/inference/classifiers.py:183\u001b[39m, in \u001b[36mEncoderClassifier.classify_file\u001b[39m\u001b[34m(self, path, **kwargs)\u001b[39m\n\u001b[32m    181\u001b[39m out_prob = \u001b[38;5;28mself\u001b[39m.mods.classifier(emb).squeeze(\u001b[32m1\u001b[39m)\n\u001b[32m    182\u001b[39m score, index = torch.max(out_prob, dim=-\u001b[32m1\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m183\u001b[39m text_lab = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mhparams\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlabel_encoder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode_torch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    184\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m out_prob, score, index, text_lab\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/mp/voice-finder/.venv/lib/python3.11/site-packages/speechbrain/dataio/encoder.py:556\u001b[39m, in \u001b[36mCategoricalEncoder.decode_torch\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    554\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m x.ndim == \u001b[32m1\u001b[39m:  \u001b[38;5;66;03m# Last dimension!\u001b[39;00m\n\u001b[32m    555\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m element \u001b[38;5;129;01min\u001b[39;00m x:\n\u001b[32m--> \u001b[39m\u001b[32m556\u001b[39m         decoded.append(\u001b[38;5;28mself\u001b[39m.ind2lab[\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43melement\u001b[49m\u001b[43m)\u001b[49m])\n\u001b[32m    557\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    558\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m subtensor \u001b[38;5;129;01min\u001b[39;00m x:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "speaker_folders = list(audio_files.iterdir())\n",
    "\n",
    "predictions = []\n",
    "\n",
    "batch_metadata = []\n",
    "batch_inputs = []\n",
    "\n",
    "# The audio data is split usnig the folder structure <speaker>/<video-clip>/<sound-clip>\n",
    "\n",
    "for speaker_folder in tqdm(speaker_folders, total=len(speaker_folders)):\n",
    "    for clip_folder in speaker_folder.iterdir():\n",
    "        for clip in clip_folder.iterdir():\n",
    "\n",
    "            prediction_raw = language_id.classify_file(str(clip))\n",
    "\n",
    "            predictions.append(\n",
    "                {\n",
    "                    \"language\": prediction_raw[3][0].split(\":\")[0],\n",
    "                    \"speaker_id\": speaker_folder.name,\n",
    "                    \"clip_id\": clip_folder.name,\n",
    "                    \"audio_file\": clip.name,\n",
    "                    \"path\": str(clip.relative_to(audio_files)),\n",
    "                }\n",
    "            )\n",
    "predictions_df = pd.DataFrame(predictions)\n",
    "\n",
    "predictions_df.to_csv(\"audio_clips_meta_data.json\")\n",
    "predictions_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
